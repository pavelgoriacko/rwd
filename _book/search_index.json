[["index.html", "Real World Data for Clinical Research 1 Preface 1.1 Credits and Acknowledgments", " Real World Data for Clinical Research Pavel Goriacko and Danielle Boyce 1 Preface Research using real-world data (RWD) is evolving at a rapid pace. A decade ago, it often meant single-institution chart reviews or loosely defined case series. Today, the landscape looks dramatically different. Advances such as common data models (CDMs), standardized vocabularies, shared repositories, powerful open-source tools (like R, Python, Git), and more sophisticated methods for causal inference and bias adjustment have transformed what is possible. Alongside these developments, we face new ethical questions about privacy, consent, transparency, and equity in the use of healthcare data. Despite this progress, much of the methodological knowledge required to conduct high-quality RWD research remains inaccessible to many clinician-researchers. The information is scattered across technical documentation, academic papers, online forums, and video lectures—often buried in mathematical notation or domain-specific jargon. For busy clinicians seeking to understand patterns in care or evaluate outcomes, this complexity can be a barrier to entry. The result is a frustrating paradox: those most familiar with the care processes we seek to study—clinicians—are often least equipped to leverage the full potential of modern RWD infrastructure. Too often, EHR-based research is equated with manual chart review, lacking systematic attention to issues like confounding, measurement error, and generalizability. Many studies fail to take advantage of harmonized, cross-institutional data sources and tools that promote transparency, reproducibility, and scalable discovery. As a result, we are missing opportunities to learn from the rich data generated every day in healthcare settings—data that, if used rigorously and ethically, could help us identify better treatments, reduce disparities, and improve patient outcomes. In response, we created this guide: a practical, clinician-oriented introduction to real-world data for clinical and translational research. Our goal is to make the field more accessible without sacrificing methodological rigor. We aim to demystify key concepts, explain the “why” behind complex workflows, and highlight modern tools and frameworks in a way that feels relevant and actionable to clinician-researchers. This book is a living project and, by nature, incomplete. We expect it to evolve over time as the field continues to grow and shift. We welcome feedback, corrections, and ideas for improvement. Our hope is that you will join us in building a shared, accessible foundation of knowledge—so that we can use the data we already have to make healthcare smarter, safer, and more equitable. 1.1 Credits and Acknowledgments (TBD) "],["introduction-to-real-world-data-rwd-and-real-world-evidence-rwe.html", "2 Introduction to Real-World Data (RWD) and Real-World Evidence (RWE) 2.1 Overview 2.2 What is Real-World Data (RWD)? 2.3 What is Real-World Evidence (RWE)? 2.4 Why is RWD Important? 2.5 Applications of RWD and RWE 2.6 Limitations and Considerations 2.7 Summary 2.8 Suggested Readings", " 2 Introduction to Real-World Data (RWD) and Real-World Evidence (RWE) 2.1 Overview This chapter introduces the foundational concepts of real-world data (RWD) and real-world evidence (RWE), including definitions, significance, key applications, and major sources. This sets the stage for understanding how RWD can be responsibly and effectively used in clinical and translational research. 2.1.1 Learning Objectives Define RWD and RWE Understand the importance of RWD in clinical and translational research Identify key applications of RWD and RWE Distinguish between different types of real-world data sources 2.2 What is Real-World Data (RWD)? Real-world data refers to data relating to patient health status and/or the delivery of health care routinely collected from a variety of sources. Common types include: Electronic Health Records (EHRs) Medical claims and billing data Product and disease registries Patient-generated data (e.g., through mobile devices or surveys) Data from digital health technologies Each have their distinct advantages and drawbacks and must be carefully evaluated for their fit to answer the research question at hand. 2.3 What is Real-World Evidence (RWE)? Real-world evidence is the clinical evidence regarding the usage and potential benefits or risks of a medical product derived from analysis of RWD. It can support: Regulatory decision-making Clinical guideline development Health technology assessments Quality improvement 2.4 Why is RWD Important? Complements traditional clinical trials by reflecting broader and more diverse patient populations Offers insights into real-world effectiveness, safety, and utilization Enables learning health systems to continuously improve care 2.5 Applications of RWD and RWE Drug safety monitoring and post-market surveillance Comparative effectiveness research Observational studies to support hypothesis generation Pragmatic clinical trials Health equity research 2.6 Limitations and Considerations Data quality and completeness Potential biases and confounding Privacy and governance issues Varying data structures and lack of standardization 2.7 Summary Real-world data and real-world evidence play an increasingly central role in modern clinical and translational research. Understanding their strengths, limitations, and appropriate applications is essential for designing impactful studies that improve health outcomes. 2.8 Suggested Readings FDA Framework for RWE NICE RWE Framework Report from the NIH Pragmatic Trials Collaboratory on EHR data use Khozin et al., 2022: Applications of RWD in oncology research "],["sources-of-real-world-data.html", "3 Sources of Real-World Data 3.1 Overview 3.2 Categories of RWD Sources 3.3 Comparing Sources 3.4 Considerations for Researchers 3.5 Summary 3.6 Suggested Readings", " 3 Sources of Real-World Data 3.1 Overview This chapter covers the primary sources of real-world data (RWD), highlighting their characteristics, strengths, limitations, and appropriate use cases in clinical and translational research. 3.1.1 Learning Objectives Understand the major categories of RWD sources. Compare the advantages and limitations of different data sources. Identify appropriate use cases for each source of RWD. Recognize the implications of data provenance for study design and interpretation. 3.2 Categories of RWD Sources 3.2.1 Electronic Health Records (EHR) Captured during routine clinical care. Structured data (e.g., lab results, diagnosis codes) and unstructured data (e.g., clinical notes). Source for detailed clinical information and longitudinal patient histories. Limitations: data quality, missingness, variation across systems, not collected for research purposes. 3.2.2 Administrative Claims Data Generated for billing and reimbursement purposes. Includes information on diagnoses, procedures, and prescriptions. National coverage (e.g., Medicare, Medicaid, private insurers). Strengths: standardization, large populations, consistent coding. Limitations: lacks clinical detail, potential miscoding. 3.2.3 Registries Disease-specific or procedure-specific data collections. Often curated with specific inclusion criteria and data standards. Examples: cancer registries, transplant registries. High-quality and structured, but may lack generalizability. 3.2.4 Patient-Generated Health Data (PGHD) Includes data from wearable devices, apps, home monitoring tools. Provides real-time insights outside of clinical settings. Challenges: data volume, reliability, integration with clinical systems. 3.2.5 Other Sources Public health surveillance databases. Social determinants of health datasets. Biobanks and genomic databases. Data from pragmatic trials and learning health systems. 3.3 Comparing Sources Data Source Strengths Limitations Best Use Cases EHR Rich clinical data, longitudinal Incomplete, messy, variable Clinical outcomes, phenotyping Claims Large scale, consistent coding Limited clinical granularity Utilization, economic outcomes Registries Focused, high-quality data Selection bias, limited population scope Quality improvement, comparative effectiveness PGHD Real-world behavior, continuous data Variable quality, integration challenges Adherence, behavior monitoring 3.4 Considerations for Researchers Provenance: Understand who collected the data, how, and for what purpose (further discussed in chapter __). Data Quality: Assess missingness, timeliness, accuracy, and validation (further discussed in chapter __). Access and Governance: Determine legal, ethical, and institutional requirements (further discussed in chapter __). Population Representativeness: Understand which groups may be underrepresented (further discussed in chapter __). 3.5 Summary Different RWD sources serve different research needs. Understanding their provenance, structure, and limitations is key to rigorous study design. Triangulation of multiple data sources may improve validity but introduces additional complexity. 3.6 Suggested Readings FDA Framework for RWD and RWE Casey et al. (2022). “Real-world data sources in oncology research.” National Academy of Medicine (2021). “Electronic Health Records and Research: Possibilities and Realities.” Gliklich et al. (Registries for Evaluating Patient Outcomes: A User’s Guide, AHRQ). "],["data-structures-for-real-world-data.html", "4 Data Structures for Real-World Data 4.1 Overview 4.2 Relational Databases 4.3 Research-Ready Datasets 4.4 EHR Architecture and Reporting Databases 4.5 Data Access and Security 4.6 Querying RWD 4.7 Data Abstraction", " 4 Data Structures for Real-World Data 4.1 Overview This chapter introduces how real-world data (RWD), especially electronic health record (EHR) and claims data, is stored and accessed. We begin with an introduction to relational databases—the standard structure for RWD—and walk through the processes by which these data are transformed into analysis-ready datasets. We then cover important topics like EHR architecture, reporting databases, query languages, and the concept of data abstraction. 4.1.1 Learning Objectives Understand the concept of relational databases and how RWD is structured in multiple related tables. Differentiate between relational databases and research-ready datasets. Describe how EHR data is replicated for reporting and research purposes. Understand the role of data models in structuring data for specific uses. Recognize security and access considerations for RWD databases. Learn about different query methods, including SQL and APIs. Understand the concept of abstraction layers in research databases. 4.2 Relational Databases Most real-world data is stored in relational databases, a structure where data is distributed across multiple related tables. For example, in an EHR system: One table may store patient demographics. Another table may contain visit information. A third table might include diagnosis codes, while a fourth has lab test results. These tables are “related” by shared keys (e.g., patient_id), allowing users to join them together to build a more complete picture of a patient’s healthcare journey. 4.3 Research-Ready Datasets Researchers are often used to working with flat files or datasets where each row represents a subject, time point, or event. These are sometimes referred to as: Long format: Each row represents a specific observation or event. Wide format: Each row represents a subject, and columns represent repeated measures or variables. To create these from relational databases, data must be queried, joined, reshaped, and often summarized. This transformation process—from a normalized relational structure to a flat, analyzable format—is one of the most common tasks in preparing RWD for research. 4.4 EHR Architecture and Reporting Databases Most EHR systems use a live production database optimized for clinical workflows and frequent read-write operations. These systems are not structured for reporting or research due to complex and performance-optimized schemas. To address this, health systems often maintain one or more replica databases: Reporting databases (e.g., Epic Caboodle) are structured to support analytics and operational reporting. Research databases may replicate and restructure data further, sometimes into a common data model (discussed in later chapters). Each database has a different data model, depending on its intended use. Production databases prioritize performance and clinical reliability; reporting databases prioritize ease of querying and summarization. 4.5 Data Access and Security Access to RWD databases is usually restricted for privacy and compliance reasons. Important considerations include: Access control: Only authorized users (e.g., institutional researchers) may have access. Network restrictions: Access often requires connection to a secure local network or VPN. Audit logs: All queries and data use may be logged for compliance. De-identification: In some cases, data is de-identified before access, though many databases contain identifiable information. 4.6 Querying RWD Most relational databases are queried using Structured Query Language (SQL). SQL allows users to select, filter, join, and aggregate data from different tables. However, not all data sources use SQL. For example: APIs (Application Programming Interfaces) may be used to retrieve data from systems that do not support direct SQL access. Elasticsearch is commonly used for querying unstructured data such as clinical notes and uses a JSON-based query structure instead of SQL. Understanding which query method applies depends on the architecture and design of the data source. 4.7 Data Abstraction To support research efficiently, many institutions develop abstraction layers—simplified versions of the full EHR database that are designed for specific research purposes. These often include: A subset of patients or data elements relevant to research. Restructured tables for easier querying and analysis. Predefined phenotypes or cohorts. This abstraction reduces complexity and enhances usability but also limits flexibility. It’s important to understand what’s included—and excluded—in these datasets. "],["common-data-models-cdms-and-standardized-vocabularies.html", "5 Common Data Models (CDMs) and Standardized Vocabularies 5.1 Overview 5.2 What is a Common Data Model? 5.3 Benefits and Tradeoffs 5.4 Examples of Common Data Models 5.5 Standardized Vocabularies 5.6 Vocabulary Mapping 5.7 Summary", " 5 Common Data Models (CDMs) and Standardized Vocabularies 5.1 Overview This chapter introduces the concept of Common Data Models (CDMs), which provide a standardized way to structure and represent real-world data across institutions. We also discuss the critical role of standardized vocabularies in enabling interoperability and reproducibility in clinical and translational research. 5.1.1 Learning Objectives Define what a Common Data Model (CDM) is and explain its purpose. Describe the advantages and tradeoffs of using a CDM in research. Compare different CDMs (e.g., OMOP, PCORnet, Sentinel, i2b2). Understand the role of standardized vocabularies in CDMs. Identify examples of commonly used clinical terminologies (e.g., SNOMED, RxNorm, LOINC, ICD, CPT). Understand how vocabularies are used to map diverse local codes to a shared semantic model. 5.2 What is a Common Data Model? A Common Data Model (CDM) is a predefined schema that structures data in a consistent format across different sites and source systems. CDMs define both the tables and fields that make up a dataset, and the relationships among them. They also specify how data should be encoded using standardized vocabularies. CDMs are designed to: Facilitate multicenter research by enabling comparable datasets. Reduce the burden of re-implementing study definitions at every site. Promote the reuse of analysis tools across studies and institutions. 5.3 Benefits and Tradeoffs Advantages: Interoperability: Different institutions can collaborate more easily using the same data structure. Reusability: Common cohort definitions and analytic code can be shared. Transparency: Standardized schemas promote clear documentation and reproducibility. Challenges: Data loss or distortion: Mapping source data to a CDM can result in loss of granularity or nuances. Implementation effort: Building and maintaining a CDM extract-transform-load (ETL) pipeline requires time and technical expertise. One-size-doesn’t-fit-all: Not every research use case fits neatly into a CDM’s structure. 5.4 Examples of Common Data Models Several CDMs are widely used in academic and regulatory settings: OMOP (Observational Medical Outcomes Partnership): Maintained by the OHDSI collaborative; focuses heavily on standardized vocabularies and reproducible analytics. PCORnet CDM: Used by the Patient-Centered Outcomes Research Network; includes more provenance and encounter-based structure. Sentinel CDM: Developed by the FDA; used for regulatory safety surveillance. i2b2: Flexible, ontology-driven model often used for cohort discovery within institutions. Each of these models has different strengths, depending on the intended use case (e.g., longitudinal safety monitoring, patient-centered research, population health, etc.). 5.5 Standardized Vocabularies To make data in a CDM meaningful and comparable across sites, CDMs rely on standardized vocabularies—sets of codes and concepts used consistently to represent clinical information. Some key examples include: SNOMED CT: For diagnoses, conditions, and clinical findings. RxNorm: For drugs and medications. LOINC: For laboratory tests and measurements. ICD-9/10: International Classification of Diseases, used for diagnoses in billing systems. CPT: Current Procedural Terminology, used for procedures and billing. These vocabularies replace local or proprietary codes with shared concepts. For instance, the same diagnosis code for “type 2 diabetes” might appear differently in two hospital systems—but both can be mapped to the same SNOMED concept in a CDM. 5.6 Vocabulary Mapping The process of converting source codes to standard concepts is known as vocabulary mapping. In OMOP, for example, local codes (such as ICD-10 or CPT) are mapped to standard concepts using a central vocabulary table. Each record in the clinical data includes both the source value and the mapped standard concept. This mapping allows researchers to write analyses that are portable across institutions, even if their source systems differ. Mapping can be complex and imperfect—some concepts have no exact match, and local customization of vocabularies (e.g., site-specific lab codes) may require manual curation. 5.7 Summary CDMs and standardized vocabularies play a crucial role in transforming heterogeneous real-world data into a harmonized format suitable for research. While implementation is complex and not without limitations, the benefits of enabling large-scale, reproducible, and collaborative research are substantial. "],["computable-phenotyping.html", "6 Computable Phenotyping 6.1 Overview 6.2 What is a Computable Phenotype? 6.3 Types of Phenotypes 6.4 Validity and Transportability 6.5 Phenotype Evaluation 6.6 Impact on Research Validity 6.7 Summary 6.8 Phenotyping Tools and Resources", " 6 Computable Phenotyping 6.1 Overview This chapter introduces the concept of computable phenotyping—the process of defining and identifying clinical concepts (e.g., diagnoses, exposures, outcomes) in real-world data using logic that can be implemented in a database query. Computable phenotypes are foundational to observational research because they determine who is included in a study, what exposures are measured, and how outcomes are identified. 6.1.1 Learning Objectives Define computable phenotyping and explain its role in observational research. Distinguish between simple code-based definitions and more complex algorithmic phenotypes. Understand the challenges in implementing reliable, valid, and transportable phenotypes. Describe common methods for evaluating phenotype validity. Identify tools and frameworks that support phenotype development and reuse. Discuss the implications of phenotype definitions for bias, validity, and reproducibility. 6.2 What is a Computable Phenotype? A computable phenotype is an algorithmic definition of a clinical state or characteristic—such as a disease, procedure, or medication exposure—that can be implemented on electronic health record (EHR) or other real-world data using code (e.g., SQL, Python, cohort definition tools). For example, a computable phenotype for diabetes might include: A diagnosis code for diabetes (e.g., SNOMED or ICD-10), A medication code for insulin or oral antidiabetics (e.g., RxNorm), Lab results such as elevated hemoglobin A1c (e.g., LOINC codes). Depending on the research question, phenotypes can be more or less stringent, and may combine multiple data domains (e.g., diagnoses + medications + labs). 6.3 Types of Phenotypes 1. Code-based Phenotypes: Defined using one or more standardized codes (e.g., SNOMED, ICD-10, RxNorm). Often implemented as concept sets or value sets. Example: A cohort of patients with a diagnosis of myocardial infarction using SNOMED codes. 2. Rule-based Phenotypes: Defined using logical combinations of codes, dates, and clinical context. Example: Two outpatient diabetes diagnoses at least 30 days apart plus an A1c ≥ 6.5%. 3. Machine Learning or NLP-derived Phenotypes: Identified using probabilistic models trained on labeled data (e.g., chart-reviewed gold standard). Often used when structured codes are inadequate or unreliable. Example: Phenotyping depression using narrative clinical notes and structured data features. 6.4 Validity and Transportability Phenotype definitions can vary in: Sensitivity: Ability to identify all true cases. Specificity: Ability to exclude non-cases. Positive Predictive Value (PPV): Probability that those identified truly have the condition. Transportability: Applicability across different institutions or datasets. Validation is critical, especially for complex or algorithmic phenotypes. Some phenotypes undergo formal validation via chart review, replication across datasets, or comparison with gold standards. Here is a revised version of the Phenotype Evaluation section that closely aligns with your notes while maintaining a clear and accessible narrative suitable for a Bookdown textbook: 6.5 Phenotype Evaluation It is essential to objectively evaluate and report the performance characteristics of a computable phenotype. This process helps determine whether the phenotype is appropriate for the intended research question and minimizes misclassification bias in downstream analyses. While many biases in observational research have established mitigation techniques, phenotype misclassification is often not adequately addressed or quantified. Later in the course, we’ll discuss how misclassification propagates as measurement error and affects causal inference. For now, the key point is that phenotype performance must be evaluated with care and transparency. 6.5.1 What Makes a Good Phenotype? A “good” phenotype is one that is: Explicit: The logic and components are clearly documented. Reproducible: It can be implemented by other researchers or in other datasets. Reliable: It yields consistent results over time or across users. Valid: It accurately reflects the intended clinical concept for its research use. Good phenotype definitions provide detail on all components, including data elements (e.g., diagnoses, labs, medications), inclusion/exclusion logic, value sets (e.g., code lists), and temporal constraints. These should be clearly articulated and shareable to ensure reproducibility. Remember: no phenotype is perfect. The goal is to achieve reasonable sensitivity and specificity, or positive/negative predictive value (PPV/NPV), depending on the research goals. Highly specific phenotypes may exclude borderline cases and reduce generalizability, while highly sensitive phenotypes may increase false positives. Researchers must choose the appropriate balance based on their use case. 6.5.2 Reliability and Validity Evaluating a phenotype typically involves comparing it to a “gold standard” — a reference set believed to represent the true clinical status of a cohort. This can involve: Manual chart review (e.g., by clinicians), Structured data proxies, or Probabilistic models trained on annotated cases. However, defining a gold standard is difficult and subject to disagreement. For example, chart reviews themselves may only achieve ~80% inter-rater reliability, as shown in studies like Ostropolets et al. Therefore, some researchers use “silver standard” methods like constructing extreme sensitivity or specificity cohorts (xSens/xSpec) to approximate ground truth in the absence of full manual review. In addition to validity (i.e., accuracy relative to a gold standard), phenotypes should also be evaluated for reproducibility across datasets and institutions. A valid phenotype that only works in one health system may not generalize well. 6.5.3 Methods of Phenotype Evaluation Several approaches are available to evaluate phenotypes: 6.5.3.1 Manual Chart Review Gold standard method. Involves clinical experts reviewing the EHR to confirm true case status. Resource-intensive and often limited to small samples. 6.5.3.2 Structured Data Review Uses other structured fields (e.g., medications, labs) to cross-check the phenotype logic. Often used in conjunction with or in place of full chart review. 6.5.3.3 Cohort Characteristics Review of descriptive statistics of the identified cohort (e.g., age, sex, comorbidities). Can highlight unexpected patterns that may signal errors or unintended logic. Example tool: OHDSI’s CohortDiagnostics package, which produces detailed reports comparing phenotype logic, code sets, and output across sites. 6.5.3.4 PheValuator A tool developed by OHDSI that uses machine learning to predict the probability of a condition based on structured data. Can estimate sensitivity, specificity, PPV, and NPV without requiring manual review. Often used in conjunction with xSens/xSpec cohorts. 6.5.3.5 Large Language Models (LLMs) Emerging technique where LLMs read through clinical notes and assess whether a patient fits a given phenotype. Can be used for sensitivity/specificity estimation, particularly for conditions not well-captured in structured data. Still under active research but offers scalable alternatives to manual review. 6.6 Impact on Research Validity Phenotype definitions have a profound effect on study validity: Misclassification can introduce measurement bias and distort associations. Differences in data availability (e.g., presence or absence of outpatient labs) can limit phenotype portability. Transparency in phenotype definitions is essential for reproducibility and peer review. Researchers should always describe phenotype logic in sufficient detail and, when possible, share code and concept sets to enable reuse. 6.7 Summary Computable phenotyping transforms messy, heterogeneous real-world data into meaningful clinical variables that drive cohort selection, exposure definitions, and outcome measurement. Designing valid, reproducible, and transparent phenotypes is a foundational skill for real-world data research and requires a mix of clinical knowledge, informatics skills, and awareness of data limitations. 6.8 Phenotyping Tools and Resources Several platforms and initiatives support development and sharing of computable phenotypes: OHDSI Atlas: A web-based tool for defining and executing cohorts using the OMOP CDM. PheKB (Phenotype KnowledgeBase): A repository of phenotyping algorithms, often for i2b2 or custom data models. eMERGE Network: A collaborative effort to develop and validate phenotypes using EHR and genomic data. Phenotype libraries: Many institutions maintain internal phenotype libraries, often built around their local data models. These tools often support cohort definition using user interfaces, standard vocabularies, and shareable JSON or SQL logic. "]]
