[["index.html", "Guide to Real World Data for Clinical Research 1 Introduction 1.1 Disclaimer 1.2 Preface 1.3 License 1.4 Credits and Acknowledgments", " Guide to Real World Data for Clinical Research Pavel Goriacko, PharmD, MPH and Danielle Boyce, DPA, MPH 1 Introduction 1.1 Disclaimer This book is a living draft and part of an ongoing effort to make real-world data (RWD) methods more accessible to clinician-researchers. We are sharing it publicly as part of a “work in public” philosophy—prioritizing openness, collaboration, and early feedback over polished perfection. As such, please be aware of the following: The content is incomplete. Many sections are still in development, and you may encounter missing examples, rough outlines, or placeholders. There may be inaccuracies. Some statements have not yet been fully reviewed or fact-checked. Do not treat this draft as a definitive reference. Formatting and organization are evolving. You may notice inconsistencies in layout, language, or style as we continue editing and revising the material. Citations may be informal or missing. While we aim to acknowledge relevant prior work, we may reference ideas or studies without formal citations in early drafts. All sources will be properly cited in a final, polished version. The content may change frequently. Sections may be restructured, rewritten, or replaced as the work progresses and as we receive input from peers and collaborators. This draft is not intended to serve as a formal educational resource or authoritative guide—at least not yet. Instead, it is a starting point for discussion. We are sharing it early because we believe that better ideas emerge when development is open, visible, and collaborative. If you have suggestions, corrections, or feedback, we’d love to hear from you. Whether you are a researcher, educator, student, or practitioner, your perspective can help us shape this book into a more useful and rigorous resource for the community. Thank you for joining us in this process. 1.2 Preface Research using real-world data (RWD) is evolving at a rapid pace. A decade ago, it often meant single-institution chart reviews or loosely defined case series. Today, the landscape looks dramatically different. Advances such as common data models (CDMs), standardized vocabularies, shared repositories, powerful open-source tools (like R, Python, Git), and more sophisticated methods for causal inference and bias adjustment have transformed what is possible. Alongside these developments, we face new ethical questions about privacy, consent, transparency, and equity in the use of healthcare data. Despite this progress, much of the methodological knowledge required to conduct high-quality RWD research remains inaccessible to many clinician-researchers. The information is scattered across technical documentation, academic papers, online forums, and video lectures—often buried in mathematical notation or domain-specific jargon. For busy clinicians seeking to understand patterns in care or evaluate outcomes, this complexity can be a barrier to entry. The result is a frustrating paradox: those most familiar with the care processes we seek to study—clinicians—are often least equipped to leverage the full potential of modern RWD infrastructure. Too often, EHR-based research is equated with manual chart review, lacking systematic attention to issues like confounding, measurement error, and generalizability. Many studies fail to take advantage of harmonized, cross-institutional data sources and tools that promote transparency, reproducibility, and scalable discovery. As a result, we are missing opportunities to learn from the rich data generated every day in healthcare settings—data that, if used rigorously and ethically, could help us identify better treatments, reduce disparities, and improve patient outcomes. In response, we created this guide: a practical, clinician-oriented introduction to real-world data for clinical and translational research. Our goal is to make the field more accessible without sacrificing methodological rigor. We aim to demystify key concepts, explain the “why” behind complex workflows, and highlight modern tools and frameworks in a way that feels relevant and actionable to clinician-researchers. This book is a living project and, by nature, incomplete. We expect it to evolve over time as the field continues to grow and shift. We welcome feedback, corrections, and ideas for improvement. Our hope is that you will join us in building a shared, accessible foundation of knowledge—so that we can use the data we already have to make healthcare smarter, safer, and more equitable. 1.3 License This book is licensed under a Creative Commons Attribution-ShareAlike (CC BY-SA 4.0) license. We want this content to be used, shared, and distributed as freely and widely as possible—while ensuring that no further restrictions can be placed on its use. If this license presents a barrier to using or adapting the material in your setting, please reach out to us; we welcome collaboration and will do our best to accommodate your needs. 1.4 Credits and Acknowledgments (TBD) "],["introduction-to-real-world-data-rwd-and-real-world-evidence-rwe.html", "2 Introduction to Real-World Data (RWD) and Real-World Evidence (RWE) 2.1 Overview 2.2 What is Real-World Data (RWD)? 2.3 What is Real-World Evidence (RWE)? 2.4 Why is RWD Important? 2.5 Applications of RWD and RWE 2.6 Limitations and Considerations 2.7 Summary 2.8 Suggested Readings", " 2 Introduction to Real-World Data (RWD) and Real-World Evidence (RWE) 2.1 Overview This chapter introduces the foundational concepts of real-world data (RWD) and real-world evidence (RWE), including definitions, significance, key applications, and major sources. This sets the stage for understanding how RWD can be responsibly and effectively used in clinical and translational research. 2.1.1 Learning Objectives Define RWD and RWE Understand the importance of RWD in clinical and translational research Identify key applications of RWD and RWE Distinguish between different types of real-world data sources 2.2 What is Real-World Data (RWD)? Real-world data refers to data relating to patient health status and/or the delivery of health care routinely collected from a variety of sources. Common types include: Electronic Health Records (EHRs) Medical claims and billing data Product and disease registries Patient-generated data (e.g., through mobile devices or surveys) Data from digital health technologies Each have their distinct advantages and drawbacks and must be carefully evaluated for their fit to answer the research question at hand. 2.3 What is Real-World Evidence (RWE)? Real-world evidence is the clinical evidence regarding the usage and potential benefits or risks of a medical product derived from analysis of RWD. It can support: Regulatory decision-making Clinical guideline development Health technology assessments Quality improvement 2.4 Why is RWD Important? Complements traditional clinical trials by reflecting broader and more diverse patient populations Offers insights into real-world effectiveness, safety, and utilization Enables learning health systems to continuously improve care 2.5 Applications of RWD and RWE Drug safety monitoring and post-market surveillance Comparative effectiveness research Observational studies to support hypothesis generation Pragmatic clinical trials Health equity research 2.6 Limitations and Considerations Data quality and completeness Potential biases and confounding Privacy and governance issues Varying data structures and lack of standardization 2.7 Summary Real-world data and real-world evidence play an increasingly central role in modern clinical and translational research. Understanding their strengths, limitations, and appropriate applications is essential for designing impactful studies that improve health outcomes. 2.8 Suggested Readings FDA Framework for RWE NICE RWE Framework Report from the NIH Pragmatic Trials Collaboratory on EHR data use Khozin et al., 2022: Applications of RWD in oncology research "],["sources-of-real-world-data.html", "3 Sources of Real-World Data 3.1 Overview 3.2 Categories of RWD Sources 3.3 Comparing Sources 3.4 Considerations for Researchers 3.5 Summary 3.6 Suggested Readings", " 3 Sources of Real-World Data 3.1 Overview This chapter covers the primary sources of real-world data (RWD), highlighting their characteristics, strengths, limitations, and appropriate use cases in clinical and translational research. 3.1.1 Learning Objectives Understand the major categories of RWD sources. Compare the advantages and limitations of different data sources. Identify appropriate use cases for each source of RWD. Recognize the implications of data provenance for study design and interpretation. 3.2 Categories of RWD Sources 3.2.1 Electronic Health Records (EHR) Captured during routine clinical care. Structured data (e.g., lab results, diagnosis codes) and unstructured data (e.g., clinical notes). Source for detailed clinical information and longitudinal patient histories. Limitations: data quality, missingness, variation across systems, not collected for research purposes. 3.2.2 Administrative Claims Data Generated for billing and reimbursement purposes. Includes information on diagnoses, procedures, and prescriptions. National coverage (e.g., Medicare, Medicaid, private insurers). Strengths: standardization, large populations, consistent coding. Limitations: lacks clinical detail, potential miscoding. 3.2.3 Registries Disease-specific or procedure-specific data collections. Often curated with specific inclusion criteria and data standards. Examples: cancer registries, transplant registries. High-quality and structured, but may lack generalizability. 3.2.4 Patient-Generated Health Data (PGHD) Includes data from wearable devices, apps, home monitoring tools. Provides real-time insights outside of clinical settings. Challenges: data volume, reliability, integration with clinical systems. 3.2.5 Other Sources Public health surveillance databases. Social determinants of health datasets. Biobanks and genomic databases. Data from pragmatic trials and learning health systems. 3.3 Comparing Sources Data Source Strengths Limitations Best Use Cases EHR Rich clinical data, longitudinal Incomplete, messy, variable Clinical outcomes, phenotyping Claims Large scale, consistent coding Limited clinical granularity Utilization, economic outcomes Registries Focused, high-quality data Selection bias, limited population scope Quality improvement, comparative effectiveness PGHD Real-world behavior, continuous data Variable quality, integration challenges Adherence, behavior monitoring 3.4 Considerations for Researchers Provenance: Understand who collected the data, how, and for what purpose (further discussed in chapter __). Data Quality: Assess missingness, timeliness, accuracy, and validation (further discussed in chapter __). Access and Governance: Determine legal, ethical, and institutional requirements (further discussed in chapter __). Population Representativeness: Understand which groups may be underrepresented (further discussed in chapter __). 3.5 Summary Different RWD sources serve different research needs. Understanding their provenance, structure, and limitations is key to rigorous study design. Triangulation of multiple data sources may improve validity but introduces additional complexity. 3.6 Suggested Readings FDA Framework for RWD and RWE Gliklich et al. (Registries for Evaluating Patient Outcomes: A User’s Guide, AHRQ). Link to PDF "],["data-structures-for-real-world-data.html", "4 Data Structures for Real-World Data 4.1 Overview 4.2 Relational Databases 4.3 Research-Ready Datasets 4.4 EHR Architecture and Reporting Databases 4.5 Data Access and Security 4.6 Querying RWD 4.7 Data Abstraction", " 4 Data Structures for Real-World Data 4.1 Overview This chapter introduces how real-world data (RWD), especially electronic health record (EHR) and claims data, is stored and accessed. We begin with an introduction to relational databases—the standard structure for RWD—and walk through the processes by which these data are transformed into analysis-ready datasets. We then cover important topics like EHR architecture, reporting databases, query languages, and the concept of data abstraction. 4.1.1 Learning Objectives Understand the concept of relational databases and how RWD is structured in multiple related tables. Differentiate between relational databases and research-ready datasets. Describe how EHR data is replicated for reporting and research purposes. Understand the role of data models in structuring data for specific uses. Recognize security and access considerations for RWD databases. Learn about different query methods, including SQL and APIs. Understand the concept of abstraction layers in research databases. 4.2 Relational Databases Most real-world data is stored in relational databases, a structure where data is distributed across multiple related tables. For example, in an EHR system: One table may store patient demographics. Another table may contain visit information. A third table might include diagnosis codes, while a fourth has lab test results. These tables are “related” by shared keys (e.g., patient_id), allowing users to join them together to build a more complete picture of a patient’s healthcare journey. 4.3 Research-Ready Datasets Researchers are often used to working with flat files or datasets where each row represents a subject, time point, or event. These are sometimes referred to as: Long format: Each row represents a specific observation or event. Wide format: Each row represents a subject, and columns represent repeated measures or variables. To create these from relational databases, data must be queried, joined, reshaped, and often summarized. This transformation process—from a normalized relational structure to a flat, analyzable format—is one of the most common tasks in preparing RWD for research. 4.4 EHR Architecture and Reporting Databases Most EHR systems use a live production database optimized for clinical workflows and frequent read-write operations. These systems are not structured for reporting or research due to complex and performance-optimized schemas. To address this, health systems often maintain one or more replica databases: Reporting databases (e.g., Epic Caboodle) are structured to support analytics and operational reporting. Research databases may replicate and restructure data further, sometimes into a common data model (discussed in later chapters). Each database has a different data model, depending on its intended use. Production databases prioritize performance and clinical reliability; reporting databases prioritize ease of querying and summarization. 4.5 Data Access and Security Access to RWD databases is usually restricted for privacy and compliance reasons. Important considerations include: Access control: Only authorized users (e.g., institutional researchers) may have access. Network restrictions: Access often requires connection to a secure local network or VPN. Audit logs: All queries and data use may be logged for compliance. De-identification: In some cases, data is de-identified before access, though many databases contain identifiable information. 4.6 Querying RWD Most relational databases are queried using Structured Query Language (SQL). SQL allows users to select, filter, join, and aggregate data from different tables. However, not all data sources use SQL. For example: APIs (Application Programming Interfaces) may be used to retrieve data from systems that do not support direct SQL access. Elasticsearch is commonly used for querying unstructured data such as clinical notes and uses a JSON-based query structure instead of SQL. Understanding which query method applies depends on the architecture and design of the data source. 4.7 Data Abstraction To support research efficiently, many institutions develop abstraction layers—simplified versions of the full EHR database that are designed for specific research purposes. These often include: A subset of patients or data elements relevant to research. Restructured tables for easier querying and analysis. Predefined phenotypes or cohorts. This abstraction reduces complexity and enhances usability but also limits flexibility. It’s important to understand what’s included—and excluded—in these datasets. "],["common-data-models-cdms-and-standardized-vocabularies.html", "5 Common Data Models (CDMs) and Standardized Vocabularies 5.1 Overview 5.2 What is a Common Data Model? 5.3 Benefits and Tradeoffs 5.4 Examples of Common Data Models 5.5 Standardized Vocabularies 5.6 Vocabulary Mapping 5.7 Summary", " 5 Common Data Models (CDMs) and Standardized Vocabularies 5.1 Overview This chapter introduces the concept of Common Data Models (CDMs), which provide a standardized way to structure and represent real-world data across institutions. We also discuss the critical role of standardized vocabularies in enabling interoperability and reproducibility in clinical and translational research. 5.1.1 Learning Objectives Define what a Common Data Model (CDM) is and explain its purpose. Describe the advantages and tradeoffs of using a CDM in research. Compare different CDMs (e.g., OMOP, PCORnet, Sentinel, i2b2). Understand the role of standardized vocabularies in CDMs. Identify examples of commonly used clinical terminologies (e.g., SNOMED, RxNorm, LOINC, ICD, CPT). Understand how vocabularies are used to map diverse local codes to a shared semantic model. 5.2 What is a Common Data Model? A Common Data Model (CDM) is a predefined schema that structures data in a consistent format across different sites and source systems. CDMs define both the tables and fields that make up a dataset, and the relationships among them. They also specify how data should be encoded using standardized vocabularies. CDMs are designed to: Facilitate multicenter research by enabling comparable datasets. Reduce the burden of re-implementing study definitions at every site. Promote the reuse of analysis tools across studies and institutions. 5.3 Benefits and Tradeoffs Advantages: Interoperability: Different institutions can collaborate more easily using the same data structure. Reusability: Common cohort definitions and analytic code can be shared. Transparency: Standardized schemas promote clear documentation and reproducibility. Challenges: Data loss or distortion: Mapping source data to a CDM can result in loss of granularity or nuances. Implementation effort: Building and maintaining a CDM extract-transform-load (ETL) pipeline requires time and technical expertise. One-size-doesn’t-fit-all: Not every research use case fits neatly into a CDM’s structure. 5.4 Examples of Common Data Models Several CDMs are widely used in academic and regulatory settings: OMOP (Observational Medical Outcomes Partnership): Maintained by the OHDSI collaborative; focuses heavily on standardized vocabularies and reproducible analytics. PCORnet CDM: Used by the Patient-Centered Outcomes Research Network; includes more provenance and encounter-based structure. Sentinel CDM: Developed by the FDA; used for regulatory safety surveillance. i2b2: Flexible, ontology-driven model often used for cohort discovery within institutions. Each of these models has different strengths, depending on the intended use case (e.g., longitudinal safety monitoring, patient-centered research, population health, etc.). 5.5 Standardized Vocabularies To make data in a CDM meaningful and comparable across sites, CDMs rely on standardized vocabularies—sets of codes and concepts used consistently to represent clinical information. Some key examples include: SNOMED CT: For diagnoses, conditions, and clinical findings. RxNorm: For drugs and medications. LOINC: For laboratory tests and measurements. ICD-9/10: International Classification of Diseases, used for diagnoses in billing systems. CPT: Current Procedural Terminology, used for procedures and billing. These vocabularies replace local or proprietary codes with shared concepts. For instance, the same diagnosis code for “type 2 diabetes” might appear differently in two hospital systems—but both can be mapped to the same SNOMED concept in a CDM. 5.6 Vocabulary Mapping The process of converting source codes to standard concepts is known as vocabulary mapping. In OMOP, for example, local codes (such as ICD-10 or CPT) are mapped to standard concepts using a central vocabulary table. Each record in the clinical data includes both the source value and the mapped standard concept. This mapping allows researchers to write analyses that are portable across institutions, even if their source systems differ. Mapping can be complex and imperfect—some concepts have no exact match, and local customization of vocabularies (e.g., site-specific lab codes) may require manual curation. 5.7 Summary CDMs and standardized vocabularies play a crucial role in transforming heterogeneous real-world data into a harmonized format suitable for research. While implementation is complex and not without limitations, the benefits of enabling large-scale, reproducible, and collaborative research are substantial. "],["computable-phenotyping.html", "6 Computable Phenotyping 6.1 Overview 6.2 What is a Computable Phenotype? 6.3 Types of Phenotypes 6.4 Validity and Transportability 6.5 Phenotype Evaluation 6.6 Impact on Research Validity 6.7 Summary 6.8 Phenotyping Tools and Resources", " 6 Computable Phenotyping 6.1 Overview This chapter introduces the concept of computable phenotyping—the process of defining and identifying clinical concepts (e.g., diagnoses, exposures, outcomes) in real-world data using logic that can be implemented in a database query. Computable phenotypes are foundational to observational research because they determine who is included in a study, what exposures are measured, and how outcomes are identified. 6.1.1 Learning Objectives Define computable phenotyping and explain its role in observational research. Distinguish between simple code-based definitions and more complex algorithmic phenotypes. Understand the challenges in implementing reliable, valid, and transportable phenotypes. Describe common methods for evaluating phenotype validity. Identify tools and frameworks that support phenotype development and reuse. Discuss the implications of phenotype definitions for bias, validity, and reproducibility. 6.2 What is a Computable Phenotype? A computable phenotype is an algorithmic definition of a clinical state or characteristic—such as a disease, procedure, or medication exposure—that can be implemented on electronic health record (EHR) or other real-world data using code (e.g., SQL, Python, cohort definition tools). For example, a computable phenotype for diabetes might include: A diagnosis code for diabetes (e.g., SNOMED or ICD-10), A medication code for insulin or oral antidiabetics (e.g., RxNorm), Lab results such as elevated hemoglobin A1c (e.g., LOINC codes). Depending on the research question, phenotypes can be more or less stringent, and may combine multiple data domains (e.g., diagnoses + medications + labs). 6.3 Types of Phenotypes 1. Code-based Phenotypes: Defined using one or more standardized codes (e.g., SNOMED, ICD-10, RxNorm). Often implemented as concept sets or value sets. Example: A cohort of patients with a diagnosis of myocardial infarction using SNOMED codes. 2. Rule-based Phenotypes: Defined using logical combinations of codes, dates, and clinical context. Example: Two outpatient diabetes diagnoses at least 30 days apart plus an A1c ≥ 6.5%. 3. Machine Learning or NLP-derived Phenotypes: Identified using probabilistic models trained on labeled data (e.g., chart-reviewed gold standard). Often used when structured codes are inadequate or unreliable. Example: Phenotyping depression using narrative clinical notes and structured data features. 6.4 Validity and Transportability Phenotype definitions can vary in: Sensitivity: Ability to identify all true cases. Specificity: Ability to exclude non-cases. Positive Predictive Value (PPV): Probability that those identified truly have the condition. Transportability: Applicability across different institutions or datasets. Validation is critical, especially for complex or algorithmic phenotypes. Some phenotypes undergo formal validation via chart review, replication across datasets, or comparison with gold standards. Here is a revised version of the Phenotype Evaluation section that closely aligns with your notes while maintaining a clear and accessible narrative suitable for a Bookdown textbook: 6.5 Phenotype Evaluation It is essential to objectively evaluate and report the performance characteristics of a computable phenotype. This process helps determine whether the phenotype is appropriate for the intended research question and minimizes misclassification bias in downstream analyses. While many biases in observational research have established mitigation techniques, phenotype misclassification is often not adequately addressed or quantified. Later in the course, we’ll discuss how misclassification propagates as measurement error and affects causal inference. For now, the key point is that phenotype performance must be evaluated with care and transparency. 6.5.1 What Makes a Good Phenotype? A “good” phenotype is one that is: Explicit: The logic and components are clearly documented. Reproducible: It can be implemented by other researchers or in other datasets. Reliable: It yields consistent results over time or across users. Valid: It accurately reflects the intended clinical concept for its research use. Good phenotype definitions provide detail on all components, including data elements (e.g., diagnoses, labs, medications), inclusion/exclusion logic, value sets (e.g., code lists), and temporal constraints. These should be clearly articulated and shareable to ensure reproducibility. Remember: no phenotype is perfect. The goal is to achieve reasonable sensitivity and specificity, or positive/negative predictive value (PPV/NPV), depending on the research goals. Highly specific phenotypes may exclude borderline cases and reduce generalizability, while highly sensitive phenotypes may increase false positives. Researchers must choose the appropriate balance based on their use case. 6.5.2 Reliability and Validity Evaluating a phenotype typically involves comparing it to a “gold standard” — a reference set believed to represent the true clinical status of a cohort. This can involve: Manual chart review (e.g., by clinicians), Structured data proxies, or Probabilistic models trained on annotated cases. However, defining a gold standard is difficult and subject to disagreement. For example, chart reviews themselves may only achieve ~80% inter-rater reliability, as shown in studies like Ostropolets et al. Therefore, some researchers use “silver standard” methods like constructing extreme sensitivity or specificity cohorts (xSens/xSpec) to approximate ground truth in the absence of full manual review. In addition to validity (i.e., accuracy relative to a gold standard), phenotypes should also be evaluated for reproducibility across datasets and institutions. A valid phenotype that only works in one health system may not generalize well. 6.5.3 Methods of Phenotype Evaluation Several approaches are available to evaluate phenotypes: 6.5.3.1 Manual Chart Review Gold standard method. Involves clinical experts reviewing the EHR to confirm true case status. Resource-intensive and often limited to small samples. 6.5.3.2 Structured Data Review Uses other structured fields (e.g., medications, labs) to cross-check the phenotype logic. Often used in conjunction with or in place of full chart review. 6.5.3.3 Cohort Characteristics Review of descriptive statistics of the identified cohort (e.g., age, sex, comorbidities). Can highlight unexpected patterns that may signal errors or unintended logic. Example tool: OHDSI’s CohortDiagnostics package, which produces detailed reports comparing phenotype logic, code sets, and output across sites. 6.5.3.4 PheValuator A tool developed by OHDSI that uses machine learning to predict the probability of a condition based on structured data. Can estimate sensitivity, specificity, PPV, and NPV without requiring manual review. Often used in conjunction with xSens/xSpec cohorts. 6.5.3.5 Large Language Models (LLMs) Emerging technique where LLMs read through clinical notes and assess whether a patient fits a given phenotype. Can be used for sensitivity/specificity estimation, particularly for conditions not well-captured in structured data. Still under active research but offers scalable alternatives to manual review. 6.6 Impact on Research Validity Phenotype definitions have a profound effect on study validity: Misclassification can introduce measurement bias and distort associations. Differences in data availability (e.g., presence or absence of outpatient labs) can limit phenotype portability. Transparency in phenotype definitions is essential for reproducibility and peer review. Researchers should always describe phenotype logic in sufficient detail and, when possible, share code and concept sets to enable reuse. 6.7 Summary Computable phenotyping transforms messy, heterogeneous real-world data into meaningful clinical variables that drive cohort selection, exposure definitions, and outcome measurement. Designing valid, reproducible, and transparent phenotypes is a foundational skill for real-world data research and requires a mix of clinical knowledge, informatics skills, and awareness of data limitations. 6.8 Phenotyping Tools and Resources Several platforms and initiatives support development and sharing of computable phenotypes: OHDSI Atlas: A web-based tool for defining and executing cohorts using the OMOP CDM. PheKB (Phenotype KnowledgeBase): A repository of phenotyping algorithms, often for i2b2 or custom data models. eMERGE Network: A collaborative effort to develop and validate phenotypes using EHR and genomic data. Phenotype libraries: Many institutions maintain internal phenotype libraries, often built around their local data models. These tools often support cohort definition using user interfaces, standard vocabularies, and shareable JSON or SQL logic. "],["study-designs-for-real-world-evidence-generation.html", "7 Study Designs for Real-World Evidence Generation 7.1 Overview 7.2 Learning Objectives 7.3 Observational Study Designs 7.4 Key Challenges in Observational Research 7.5 Cohort Studies 7.6 Target Trial Emulation (TTE) 7.7 The ATLAS Framework 7.8 Addressing Bias and Threats to Validity 7.9 Summary", " 7 Study Designs for Real-World Evidence Generation 7.1 Overview Once a computable phenotype has been defined and validated, the next step in a real-world data (RWD) study is to apply that phenotype in the context of a well-designed research question. The goal is to generate valid, reliable real-world evidence (RWE) from observational data. This chapter introduces foundational study designs and frameworks that help researchers minimize bias and enhance the validity of their findings. We focus primarily on cohort studies using logic-based phenotyping, which are the most common designs in RWD research. We also introduce basic graphical notation (e.g., DAGs) to illustrate key concepts. 7.2 Learning Objectives Differentiate between major observational study designs. Describe key sources of bias in observational research. Explain the importance of the target trial emulation (TTE) framework. Understand how cohort studies are constructed in ATLAS. Identify techniques to mitigate confounding, selection bias, and measurement error. 7.3 Observational Study Designs There are several types of observational studies, each suited to different research questions and data contexts: Cross-sectional studies: Assess exposure and outcome at a single point in time. Prospective cohort studies: Follow individuals over time from a defined starting point. Retrospective cohort studies: Use historical data to define cohorts and follow-up periods. Case-control studies: Identify subjects with an outcome of interest (cases) and compare their prior exposures to those without the outcome (controls). Among these, cohort studies are the most commonly used in real-world evidence generation, particularly when using common data models and tools like OHDSI’s ATLAS. 7.4 Key Challenges in Observational Research Unlike randomized trials, observational studies face several threats to internal validity. The most common sources of bias include: Confounding: Differences in baseline characteristics between exposed and unexposed groups that affect outcomes. Selection bias: Systematic differences in who is included or excluded from the cohort. Measurement error: Misclassification of exposures, outcomes, or covariates, often due to limitations of real-world data sources or phenotype definitions. 7.5 Cohort Studies A cohort is a set of individuals who meet one or more inclusion criteria over a defined period. Cohort studies observe these individuals forward in time to assess the occurrence of outcomes. Importantly: A clinical trial is a type of cohort study with randomized exposure. In observational research, exposures are not randomly assigned, so careful design is essential to draw valid conclusions. 7.5.1 Graphical Representation Cohort entry and time-at-risk can be visualized using timelines. The following image shows how events are anchored to the earliest qualifying event, which determines cohort entry: EarliestEventExplained.png 7.6 Target Trial Emulation (TTE) The target trial emulation framework encourages researchers to explicitly design their observational studies to mimic a hypothetical randomized trial. This includes defining: Eligibility criteria Treatment strategies Assignment procedures (hypothetical, in the observational context) Follow-up period Outcome definition Causal contrast of interest Analysis plan TTE is especially useful for reducing selection bias and clarifying the timing of exposures and outcomes—often referred to as time zero. 7.7 The ATLAS Framework OHDSI’s ATLAS platform provides a visual interface for constructing cohort-based studies. It enables: Defining exposure and outcome cohorts using logic-based phenotyping. Specifying time-at-risk windows. Visualizing inclusion/exclusion criteria. Exporting study specifications for execution in distributed network studies. ATLAS helps standardize study designs and encourages reproducibility by aligning closely with the TTE framework. 7.8 Addressing Bias and Threats to Validity 7.8.1 Selection Bias Choose time zero carefully (e.g., date of diagnosis vs. date of treatment). Use the TTE framework to clearly define eligibility and avoid immortal time bias. Assess how inclusion/exclusion criteria affect generalizability. 7.8.2 Confounding Identify confounders using domain expertise and tools like DAGs. Address confounding using: Stratification Matching (e.g., propensity score) Multivariable regression Inverse probability of treatment weighting (IPTW) 7.8.3 Measurement Error Evaluate the performance characteristics of phenotype definitions. Use sensitivity and specificity estimates to adjust results or interpret limitations. Conduct quantitative bias analysis when feasible. 7.8.4 Additional Tools Sensitivity analyses: Test robustness of results under varying assumptions. P-value calibration: Adjust for multiple testing and observational bias. Positive and negative controls: Benchmark your design against known outcomes. 7.9 Summary Designing a high-quality RWD study requires more than defining a cohort—it demands attention to bias, careful emulation of randomized trials, and thoughtful evaluation of validity. Cohort studies, supported by the TTE framework and tools like ATLAS, provide a powerful foundation for generating credible real-world evidence when designed and executed rigorously. "],["data-science-toolkit-for-real-world-data.html", "8 Data Science Toolkit for Real-World Data 8.1 Overview 8.2 Learning Objectives 8.3 Why Learn Modern Tools? 8.4 Core Tools: R, SQL, and Git 8.5 Data Science in Practice 8.6 Summary", " 8 Data Science Toolkit for Real-World Data 8.1 Overview Modern real-world data (RWD) research is built on a foundation of computational tools and workflows. Gone are the days when datasets could be opened in Excel, manually cleaned, and exported to statistical software. Today’s research increasingly takes place within secure environments, uses massive datasets that cannot be downloaded, and relies on code-based workflows to ensure transparency, reproducibility, and collaboration. This chapter introduces the essential tools used in real-world evidence (RWE) generation—R, SQL, and Git—and explains why they are central to the future of clinical and translational research. 8.2 Learning Objectives Explain why traditional tools like Excel are not sufficient for working with RWD. Describe the benefits of code-based, literate programming approaches. Identify the roles of R, SQL, and Git in data science workflows. Understand the importance of reproducibility, collaboration, and auditability in research. Introduce the concept of “tidy data” and the role of data wrangling in analysis. 8.3 Why Learn Modern Tools? 8.3.1 The Limits of Excel Excel is still the most commonly used tool for data management in clinical research, but it introduces several risks and limitations, especially when working with complex, high-volume RWD: It can alter data types, dates, or numeric precision (e.g., converting gene names into dates). It has row limits and performance issues with large files. Excel files are not standardized—formatting and behavior may differ by version or user. It does not support reproducible workflows: each analysis step is hidden and difficult to trace or reproduce. 8.3.2 Working with Remote Data Repositories Most RWD is stored on secure servers due to patient privacy concerns. For this reason, researchers are not allowed to download data to their local machines. Instead, they must: Connect remotely to a database. Write code (usually in SQL and R) to extract and analyze data. Work within institutional firewalls or virtual environments that are audited and access-controlled. Many tools—especially those from the OHDSI community—are designed specifically for these environments. For example, OHDSI’s entire analytics stack is written in R, hosted on GitHub, and supports standardized analyses across distributed data networks. 8.3.3 Literate Programming and Reproducible Research Modern tools support literate programming, a principle where code and narrative are woven together in a transparent, human-readable form (e.g., R Markdown). This approach enables: Reproducibility: Anyone can re-run the code and get the same results. Auditability: All steps are documented and can be reviewed. Transparency: Decisions, assumptions, and logic are clearly stated. Open Science: Sharing your code allows others to critique, reuse, and extend your work. 8.3.4 Collaboration Real-world research is often collaborative and distributed. Using code-based tools allows researchers to: Share workflows, packages, and functions across institutions. Use platforms like GitHub to track contributions and review changes. Participate in community-based initiatives like OHDSI, which provides open-source tools for RWD analytics. 8.4 Core Tools: R, SQL, and Git 8.4.1 R R is a programming language designed for data analysis and statistical computing. It has become one of the most widely used tools in RWD research, alongside Python. Researchers use RStudio, an integrated development environment (IDE), to write, debug, and run R code. R’s strength lies in its ecosystem of packages—modular libraries for importing, cleaning, analyzing, and visualizing data. Most OHDSI tools (e.g., CohortDiagnostics, PatientLevelPrediction) are written in R. 8.4.2 SQL SQL (Structured Query Language) is the standard language for querying relational databases. It allows you to extract specific data elements from large datasets based on precise criteria. SQL is declarative: you describe what data you want, and the database engine figures out how to get it. While SQL syntax is generally consistent, different databases have dialects (e.g., PostgreSQL, Oracle, SQL Server). OHDSI’s SqlRender package helps standardize code across dialects. Understanding database schemas and standard vocabularies is essential for writing effective SQL queries in RWD contexts. 8.4.3 Git Git is a version control system that tracks changes to code and documents over time. GitHub (or GitLab) is a platform where researchers can host, share, and collaborate on code. Benefits of using Git: Revert to prior versions of a script. Collaborate with others asynchronously. Submit or review code contributions (pull requests). Document work as it evolves. Thank you — your structure is very clear and aligns well with how real-world data science is actually practiced. Below is a revised version of the “Data Science in Practice” section that integrates your suggestions more explicitly. It emphasizes the two major analytic approaches up front, organizes the workflow into steps, and adds detail on what each path looks like in practical terms. 8.5 Data Science in Practice 8.5.1 Two Analytic Approaches: Standardized vs Custom Workflows Real-world data science involves navigating large, complex, and sensitive clinical datasets in a way that produces valid, reproducible, and meaningful evidence. To do this effectively, researchers tend to follow one of two analytic approaches: Standardized Analytics Pipelines These use pre-defined, validated tools (like OHDSI’s HADES packages) that encode best practices into reproducible study packages. They require minimal custom coding and are especially useful for large-scale studies or network research across multiple institutions. Custom Data Science Workflows These offer greater flexibility and granularity, allowing researchers to define custom analyses in R or Python. These workflows are often used in exploratory studies, hypothesis generation, or when unique study designs or data transformations are required. Understanding when and how to use each approach is critical. Most projects begin with a similar first step and then diverge depending on the needs of the research question and the maturity of the available tools. 8.5.2 Step 1: Foundation-Building Tasks Shared by All Approaches Regardless of the analytic pathway chosen, the initial stages of a real-world data (RWD) study generally include: Assessing the Data Source Evaluating data completeness, timing, availability of key variables, and general fitness for answering the research question. Phenotype Development and Evaluation Creating reproducible logic for defining exposures, outcomes, inclusion/exclusion criteria, and covariates, followed by evaluation of their performance characteristics (see earlier chapter on computable phenotyping). Cohort Logic Construction Translating the inclusion/exclusion logic into code that can be applied to the database, either manually (via SQL or cohort definition tools) or automatically (via platforms like OHDSI ATLAS). Once these foundations are in place, the researcher decides between a standardized or custom approach to analysis. 8.5.3 Step 2A: Standardized Analytics In a standardized workflow, the researcher either: Creates a new study package using tools like OHDSI ATLAS and HADES, or Adopts an existing study package (e.g., from GitHub or the OHDSI community) These packages follow a specific structure and allow the researcher to: Define the cohorts and covariates using standard concept sets and cohort definitions. Point the package to the local OMOP CDM database by editing a single configuration file (usually settings.json or Renviron). Specify all study parameters in one place, including cohort IDs, inclusion criteria, outcome definitions, time-at-risk windows, analysis strategies, and more. Run the study locally or remotely on a secure database, without needing to extract raw data. The output typically includes: Baseline characteristics (e.g., demographics, comorbidities) Cohort attrition and flow diagrams Effect estimates and confidence intervals Diagnostic plots and tables for study evaluation This approach minimizes programming burden, promotes reproducibility, and allows institutions to share results without sharing patient-level data. 8.5.4 Step 2B: Custom Data Science Workflows When more flexibility is needed—such as in early-phase exploratory work, novel study designs, or advanced modeling—researchers take a more customized approach. This involves: Extracting the cohort from the database using SQL or cohort-building tools. Tidying and transforming the data, including: Filtering for relevant time periods Joining tables (e.g., demographics, drug exposures, diagnoses) Creating derived variables or longitudinal features Validating the data, checking for missingness, implausible values, or inconsistencies. Conducting analyses using R packages tailored for: Descriptive statistics Prediction modeling Causal inference (e.g., propensity score matching, regression, survival analysis) Documenting the workflow using tools like RMarkdown and version control (e.g., GitHub) to ensure transparency and reproducibility. While this approach offers greater flexibility, it requires technical fluency and careful attention to reproducibility and versioning. 8.6 Summary In RWD research, most of the work occurs before traditional statistical analysis begins. From evaluating data sources and developing phenotypes to deciding on a workflow path, these tasks require both methodological rigor and technical skills. Two major pathways—standardized analytic pipelines and custom programming workflows—serve different roles. Standardized tools like OHDSI HADES make it easy to run consistent, reproducible studies across institutions, while custom workflows allow for flexibility and innovation. A successful RWD researcher understands both approaches and can choose the right tool for the right task. "],["working-with-data-repositories.html", "9 Working with Data Repositories 9.1 Overview 9.2 What Are Data Repositories? 9.3 The Repository Data Lifecycle 9.4 Gaining Access: Common Processes 9.5 Evaluating Repository Fit for Your Study 9.6 Examples of Major Data Repositories 9.7 Summary", " 9 Working with Data Repositories 9.1 Overview As real-world data (RWD) research continues to grow, centralized data repositories have become critical infrastructure for enabling large-scale, multi-institutional studies. These repositories bring together data from multiple health systems, harmonize it using shared standards, and provide secure access for analysis—offering immense opportunities, but also posing logistical and methodological challenges. This chapter introduces the structure and function of data repositories, describes how to access and evaluate them, and provides guidance for selecting an appropriate repository for your research question. 9.2 What Are Data Repositories? Data repositories are large, curated databases that aggregate health information from multiple institutions—sometimes regionally, sometimes nationwide. Examples include the NIH’s All of Us Research Program, the National COVID Cohort Collaborative (N3C), and commercial platforms like Epic Cosmos. Repositories are typically built around common data models (CDMs), such as OMOP or PCORnet CDM, to enable standardized analytics. Data may come from EHRs, insurance claims, laboratory systems, and external linkages such as mortality data or census information. 9.2.1 Advantages of Using Repositories Larger, More Representative Populations Access to data from across institutions enables studies with more generalizable results. More Complete Patient Records If a patient receives care at multiple locations, a repository may help unify their longitudinal data. Standardization and Harmonization Data are mapped to standard vocabularies and formats, allowing for easier study design and reproducibility. Analytic Tools and Collaboration Infrastructure Many repositories offer built-in analysis environments, shared protocols, and cross-institutional research communities. 9.2.2 Challenges and Limitations Access and Governance Access requires registration, training, IRB review, and approval of a formal data use request (DUR). This process can take weeks to months. Deidentification and Reidentification Constraints Most repositories contain deidentified data, limiting researchers’ ability to trace records back to local EHRs for verification. Harmonization Complexities Differences in source data formats, vocabularies, and quality across sites require careful translation and cleaning. Restricted Analysis Environments Data often reside within secure enclaves; they cannot be downloaded, and all computation must be conducted remotely. 9.3 The Repository Data Lifecycle Repositories implement a series of steps to ensure that contributed data are usable, privacy-preserving, and analytically sound. 9.3.1 Harmonization Common Data Models Source data are transformed into a standardized format (e.g., OMOP, PCORnet) and mapped to shared vocabularies like SNOMED, RxNorm, and LOINC. Terminology Translation Site-specific codes are converted to standard concepts. When multiple vocabularies exist (e.g., both LOINC and SNOMED for labs), consistent conventions must be adopted. Unit Standardization Numerical values (e.g., weight in lbs vs kg) are converted to standard units. 9.3.2 Data Quality Assurance Checks for Internal Consistency These include: plausible distributions of age and visit dates, no negative hospital length-of-stay, and expected domain-specific record counts. Site Scorecards Repositories often provide feedback to contributing institutions on data completeness and conformity. 9.3.3 Deduplication Cross-Institution Matching Patient data from multiple sources are linked and deduplicated when possible. 9.3.4 Deidentification HIPAA Safe Harbor Techniques Includes removal of direct identifiers, truncation of ZIP codes, and generalization of rare diagnoses. Date Shifting Dates are often shifted ±180 days (or ±180 minutes for time stamps) to preserve temporal patterns while obscuring exact dates. Levels of Deidentification Fully deidentified datasets remove all 18 HIPAA identifiers. Limited deidentified sets retain dates and ZIP codes but are subject to restricted access. Synthetic data mimics real data patterns without using actual patient records. 9.3.5 Privacy-Preserving Linkage Some repositories support linkage to other datasets (e.g., CMS claims, social security mortality, genomic data) without revealing individual identities. 9.4 Gaining Access: Common Processes Although the specifics vary across repositories, most follow a general set of access steps: Registration and Account Creation Researchers must register and often affiliate with a participating institution. Training Requirements Mandatory training includes responsible data use, privacy protections, and use of analytic platforms. Data Use Agreements (DUAs) Institutional DUAs govern the relationship between your institution and the repository. Individual DUAs define the responsibilities of each researcher. Project Registration Most studies require documentation of IRB approval, named collaborators, and data use limitations. Data Use Request (DUR) A formal request to access data for a specific project. This includes cohort definitions, outcomes, and planned analyses. Virtual Workspaces Approved users are granted access to secure computing environments that support SQL, R, and Python. Code and output stay within the enclave and may be audited. Collaboration Tools Many repositories offer forums, project registries, and shared communication channels (e.g., monthly calls, Slack) to foster community and reuse. ⚠️ Note: Not all repositories include every one of these features. Highly funded platforms like All of Us and N3C offer comprehensive toolsets, while others may be limited in scope or infrastructure. 9.5 Evaluating Repository Fit for Your Study Before committing to a repository-based project, it is essential to assess whether the available data are suitable for your research question. This process typically involves: 9.5.1 1. Define Your Study Needs Cohort Criteria Outcomes of Interest Key Confounders and Covariates 9.5.2 2. Assess General Fit Data Sources Does the repository include EHR, claims, patient-reported data, or external linkages? Population and Disease Focus Some repositories are disease-specific (e.g., cancer, COVID-19), while others are general. Deidentification Impact Date shifting or ZIP truncation may limit your ability to compute certain metrics or link to external datasets. Scope of Data Submission Does the repository include inpatient, outpatient, lab, pharmacy data? What is the data span over time? 9.5.3 3. Conduct Feasibility and Completeness Checks Patient Counts Estimate how many patients meet basic inclusion criteria. Critical Variable Completeness Confirm that key variables (e.g., diagnoses, drug exposures) are well-populated. Proxy Variables for Missing Data Some elements like social needs or behaviors may be missing but can be estimated via ZIP code or demographics. Follow-up and Longitudinality Assess how complete the longitudinal data are: Are there regular follow-up visits, lab results, or medication refills? 9.6 Examples of Major Data Repositories Repository Coverage Data Model Notes N3C National (COVID-focused) OMOP Includes linkages (e.g., mortality, viral variants) All of Us National, all diseases OMOP Includes genomic data, EHR, surveys, and wearable/device data Epic Cosmos Commercial (Epic customers) Cosmos CDM Includes both deidentified and limited datasets; proprietary access PCORnet National (multi-institutional) PCORnet CDM Designed for distributed queries and pragmatic trials INSIGHT New York regional consortium OMOP Aggregates NYC academic health system data OpenSAFELY UK-based national repository Custom Secure analytics on pseudonymized NHS data within the NHS firewall Optum U.S. commercial claims + EHR Proprietary Combines claims and clinical data; commonly used for pharmacoepidemiology MarketScan U.S. commercial and Medicaid claims Proprietary Includes employer-sponsored insurance claims; large sample sizes 9.7 Summary Working with data repositories opens new doors for high-impact, large-scale RWD research. But it also introduces new responsibilities: understanding data provenance, navigating governance, and ensuring that data are fit for purpose. A successful repository-based study starts with a clear definition of data needs and ends with a thoughtful analysis in a secure, reproducible computing environment. Choose the right repository, follow the appropriate access steps, and make sure the data can support your question. "],["ethical-issues-in-real-world-data-use.html", "10 Ethical Issues in Real-World Data Use 10.1 Methodological Bias and Data Integrity 10.2 Representation and Equity 10.3 Algorithmic Bias 10.4 Privacy and Data Protection 10.5 Informed Consent, Social License, and Public Trust", " 10 Ethical Issues in Real-World Data Use Real-world data (RWD) offers tremendous promise for advancing clinical and translational research, but it also introduces a range of ethical challenges that differ from those in traditional randomized controlled trials (RCTs). RWD is observational by nature and is often collected for clinical, administrative, or operational purposes—not research. As such, it may be incomplete, inconsistently recorded, or subject to numerous biases. Unlike RCTs, most RWD studies do not obtain informed consent from individuals, which opens up additional challenges related to privacy, transparency, and community trust. This chapter explores the ethical dimensions of working with RWD, including methodological rigor, representation and equity, privacy, algorithmic bias, and public trust. Because RWD tends to reflect healthcare systems in wealthy, industrialized countries, and primarily includes individuals who access care at academic or large healthcare institutions, it can systematically exclude populations who face barriers to care, stigmatization, or live in under-resourced regions. Ethical stewardship of RWD therefore requires both rigorous methodological practices and community-centered governance frameworks. 10.1 Methodological Bias and Data Integrity Observational research using RWD is more prone to confounding, selection bias, and missing data compared to RCTs: Confounding: Failing to adequately control for variables associated with both treatment selection and outcomes can lead to incorrect conclusions. For instance, early observational studies suggested a benefit of vitamin E supplementation for cardiovascular health, but these findings were later contradicted by RCTs. Emulating target trials and applying advanced methods such as inverse probability weighting or instrumental variables can mitigate this. Selection bias: This occurs when study populations differ from the target population. A notable example is hormone replacement therapy, where initial observational data (e.g., Nurses’ Health Study) showed benefit, but later RCTs (e.g., Women’s Health Initiative) showed harm—partly because observational studies selected for healthy, adherent patients. Designing studies with representative inclusion criteria and validating findings across datasets can help address this. Missing data: Missingness is not random—it often reflects disparities in access to care. For example, an undocumented health condition may indicate the event never happened—or that the person lacked access to care, and the event went unrecorded. Imputation strategies and sensitivity analyses can reduce bias but should be carefully justified. Data dredging (or p-hacking): Because RWD studies do not typically require protocol registration or pre-specification of endpoints, researchers can be tempted (intentionally or not) to analyze data in ways that confirm their hypotheses. Even when studies are pre-registered, nothing prevents researchers from exploring outcomes using different methodologies before finalizing the protocol. Strategies to address this include committing to open science practices such as pre-registration, publishing study protocols and analytic code, and using the framework of target trial emulation to define methods before seeing results. 10.2 Representation and Equity RWD can reinforce structural inequities in healthcare: Systematic exclusion: Patients with poor access to care, stigmatized conditions, or who avoid healthcare systems are often missing from RWD. This limits generalizability and may entrench disparities. Researchers should assess population coverage and consider linking to supplementary data sources to improve inclusiveness. Non-representative sampling: RWD often reflects WEIRD populations—Western, Educated, Industrialized, Rich, and Democratic. Non-English speakers, rural populations, and those without insurance are often underrepresented. Collaborating with diverse healthcare settings and applying sampling weights or subgroup analyses can help. Lack of detailed demographic data: Important dimensions of identity—such as language preference, sexual orientation, or multi-racial identity—are often incompletely or inaccurately captured, limiting the ability to study health disparities. Researchers should transparently describe data limitations and advocate for more inclusive data collection. 10.3 Algorithmic Bias Machine learning models built on RWD can reproduce and amplify existing biases: Biases in proxy variables: Convenient surrogates for outcomes or health needs can inadvertently encode structural inequities. For example, a widely cited study by Obermeyer et al. showed that healthcare spending was used as a proxy for health needs, but at the same risk score, Black patients were significantly sicker than White patients. Careful validation of proxies and consultation with domain experts and community members can mitigate this risk. Reinforcing unequal access: Predictive algorithms trained on historical care patterns may limit access to future care for already-marginalized groups. Transparency about algorithm design, public model documentation, and impact audits can help ensure fairness. 10.4 Privacy and Data Protection The sensitivity of health data raises serious privacy concerns: Risk of re-identification: Even deidentified data can sometimes be re-identified, especially when combined with other data sets. This is particularly concerning for stigmatized conditions such as HIV, disability, or autism. Applying rigorous deidentification techniques (e.g., date shifting, truncation of ZIP codes) and conducting re-identification risk assessments are important safeguards. Misuse by third parties: Health data could be accessed or purchased by insurers, employers, or government agencies and used to discriminate. Clear data use agreements and access policies are essential. Building public trust: It is essential to foster trust through rigorous data stewardship and governance processes that are transparent and inclusive. Institutions should communicate openly about data protections and provide mechanisms for concerns or opt-outs where feasible. 10.5 Informed Consent, Social License, and Public Trust Most RWD studies do not obtain informed consent from individuals, relying instead on waivers approved by institutional review boards (IRBs) under regulations like 45 CFR 46.116(d). These waivers are granted when the research poses minimal risk and cannot practicably be carried out with consent due to scale. However, this regulatory framework does not fully address broader social and ethical concerns. Patients may object to their data being used for politically motivated or stigmatizing research, even if deidentified and IRB-approved. This raises the issue of social license—the informal but essential approval from the public that data use aligns with community values and expectations. Empowering communities: Large-scale data use should involve representative stakeholders in governance decisions, especially when studying sensitive or marginalized populations. Community advisory boards and patient representatives can guide research design and ensure cultural sensitivity. Preventing social harm: Even deidentified data use can perpetuate stigma or discrimination, especially if the research implies that certain identities or conditions are pathological. Ethical review should assess not only individual risk but also social harm. Transparency: Researchers must be clear about how data is used, shared, and protected—and ensure that studies do not inadvertently undermine trust in the healthcare system. Public-facing summaries and open communication can enhance accountability. 10.5.1 Case Study: Autism Database Proposal In 2024, Health Secretary Robert F. Kennedy Jr. faced criticism for his plan to create a national autism registry. While registries are common in RWD research, this proposal raised alarm due to its framing of autism as a disease to be tracked, fueling fears of stigma and surveillance. The controversy highlights several key ethical concerns: Framing and harm: Labeling autism as a preventable disease can undermine the neurodiversity movement and harm people who identify as autistic. Consent and governance: Many individuals do not want their data used for politically charged or scientifically unsupported research. Deidentification and fear: Even if data is deidentified, there were concerns that individuals with autism could be tracked or targeted. Reflection Question: Should individuals have a say in how their health data is used—even if the data is deidentified and the research is IRB-approved? How can we ensure ethical oversight for politically or socially sensitive studies? Real-world data holds enormous potential to advance health and equity—but only if used ethically and responsibly. This requires not only technical excellence but also moral clarity, humility, and a commitment to the communities behind the data. "],["expanding-the-boundaries-of-real-world-data-special-topics-and-emerging-frontiers.html", "11 Expanding the Boundaries of Real-World Data: Special Topics and Emerging Frontiers 11.1 Rare Diseases and Small Populations 11.2 External Controls and Synthetic Comparators 11.3 Wearables and Device-Generated Data 11.4 Underrepresented Clinical Domains 11.5 High-Dimensional and Non-Tabular Data Types 11.6 Clinical Notes and Unstructured Text", " 11 Expanding the Boundaries of Real-World Data: Special Topics and Emerging Frontiers Real-world data (RWD) research is evolving rapidly, encompassing an expanding array of clinical domains, data types, and methodological innovations. This chapter explores specialized contexts where RWD is either uniquely promising or especially challenging. These include rare diseases, synthetic controls for single-arm trials, wearable devices, underrepresented clinical areas, high-dimensional data types like waveforms and imaging, and unstructured text from clinical notes. Each section highlights both the potential and the pitfalls of working in these domains, along with practical and ethical considerations for researchers. 11.1 Rare Diseases and Small Populations Rare diseases present distinct challenges for RWD research: Misdiagnosis and underreporting: Rare conditions are frequently misdiagnosed or miscoded, leading to fragmented or inaccurate data. Limited data availability: Most RWD sources lack the sample size needed for robust statistical analysis. Reliance on registries: Rare disease studies often depend on disease-specific registries or patient advocacy datasets, which may not be linked to broader EHR systems. Researchers can address these challenges by: Sharing data across sites and building federated networks Using Bayesian or hierarchical models to borrow strength across subgroups Integrating genomic or family history data to refine case identification 11.2 External Controls and Synthetic Comparators RWD can be used to construct external control arms for single-arm clinical trials, particularly in oncology and rare diseases: Approaches: Propensity score matching, synthetic controls, and digital twins aim to create comparable non-intervention groups. Risks: Lack of protocol harmonization, unmeasured confounding, and differing outcome definitions can compromise validity. To improve rigor: Pre-specify endpoints and inclusion criteria aligned with the trial Conduct sensitivity analyses and transparency reporting Use regulatory guidance to ensure acceptability 11.3 Wearables and Device-Generated Data Wearable sensors and devices produce continuous, high-frequency data on activity, vital signs, and behaviors: Challenges: Data is typically generated outside clinical settings and stored in proprietary formats There is often no standard way to integrate it into the EHR Quality can vary based on context and user behavior Opportunities: Detect early clinical deterioration Understand behavior and adherence Supplement traditional clinical endpoints with real-time patient-generated data Researchers should consider working with informaticians and engineers to develop pipelines for cleaning, summarizing, and contextualizing device data. 11.4 Underrepresented Clinical Domains Certain clinical areas are poorly represented in mainstream RWD research: Dentistry: Often stored in separate systems with limited linkage to medical EHRs Ophthalmology: Relies heavily on imaging and specialized documentation tools Behavioral health: Data may be narrative, coded differently, or subject to higher privacy protections To make progress: Use domain-specific vocabularies and standards (e.g., SNODENT, DICOM) Work with specialists to interpret data in context Explore crosswalks and linkage strategies between clinical domains 11.5 High-Dimensional and Non-Tabular Data Types RWD is increasingly incorporating non-tabular data: Waveform data: EKGs, EEGs, and respiratory signals often stored as raw time-series data Imaging data: CT, MRI, and retinal imaging can contain rich diagnostic information These data types require: Specialized infrastructure for storage and compute Advanced analysis methods (e.g., signal processing, computer vision, deep learning) Integration with clinical context for labeling and interpretation 11.6 Clinical Notes and Unstructured Text Free-text clinical documentation contains essential insights not captured in structured fields: Advantages: Provides reasoning, nuance, and context Captures social, behavioral, and environmental factors Challenges: Inconsistent language, misspellings, and abbreviations Difficult to deidentify reliably Requires NLP techniques for extraction Effective use of clinical notes involves: Applying natural language processing (NLP) methods such as named entity recognition, negation detection, and temporal reasoning Validating text-derived features against structured data Balancing extraction quality with privacy protection "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
